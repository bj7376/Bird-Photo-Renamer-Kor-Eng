# íŒŒì¼ ì´ë¦„: core_logic.py (CSV + Wikipedia ìµœì¢… ì™„ì„±ë³¸)
"""
Gemini ì´ë¯¸ì§€ ë¶„ì„ + Wikipedia + ì‚¬ìš©ì CSV(í•™ëª…â†’êµ­ëª…) ë³´ì™„ìœ¼ë¡œ
ì¡°ë¥˜ ì‚¬ì§„ ìë™ ë¶„ë¥˜, íŒŒì¼ëª… ë³€ê²½, íƒì¡° ê¸°ë¡ ìƒì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import json
import os
import re
import shutil
import time
from datetime import datetime
from typing import Callable, Dict, List

import pandas as pd
from PIL import Image

# ---------------------- ìœ í‹¸ë¦¬í‹° ----------------------

def sanitize_filename(name: str) -> str:
    if not isinstance(name, str):
        return ""
    name = name.replace('*', '')
    name = re.sub(r'[\\/:"*?<>|]', '', name).strip()
    return re.sub(r"\s+", "_", name)


def get_photo_datetime(img: Image.Image):
    try:
        exif = img._getexif()  # type: ignore
        if not exif:
            return None
        ds = exif.get(36867) or exif.get(306)
        return datetime.strptime(ds, "%Y:%m:%d %H:%M:%S") if ds else None
    except Exception:
        return None

# ------------------ ì™¸ë¶€ ë°ì´í„° ì¡°íšŒ ------------------

def wiki_lookup(wiki, common: str | None, sci: str | None, log):
    if not common:
        return None
    log(f"  - Wikipediaì—ì„œ '{common}' ê²€ìƒ‰ ì¤‘...")
    page = wiki.page(common)
    if not page.exists() and sci:
        log(f"  - ì˜ë¬¸ëª… ì‹¤íŒ¨, í•™ëª… '{sci}'ë¡œ ì¬ê²€ìƒ‰...")
        page = wiki.page(sci)
    if page.exists():
        en = page.title
        ko = page.langlinks.get('ko').title if page.langlinks and 'ko' in page.langlinks else f"*{en}"
        log(f"  - Wikipedia ì°¾ìŒ: {ko} | {en}")
        return {"korean_name": ko, "common_name": en}
    log("  - Wikipedia ê²°ê³¼ ì—†ìŒ.")
    return None


def csv_lookup(csv_df: pd.DataFrame | None, sci: str | None, log):
    if csv_df is None or not sci:
        return None
    
    # CSV êµ¬ì¡° í™•ì¸: ì»¬ëŸ¼ëª…ì´ ìˆëŠ”ì§€ ë˜ëŠ” ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
    try:
        # ì»¬ëŸ¼ëª…ì´ ìˆëŠ” ê²½ìš° ì‹œë„
        if "í•™ëª…" in csv_df.columns and "êµ­ëª…" in csv_df.columns:
            mask = csv_df["í•™ëª…"].str.strip().str.lower() == sci.strip().lower()
            if mask.any():
                ko = csv_df.loc[mask, "êµ­ëª…"].iloc[0]
                log("  - CSV ì¼ì¹˜ í•­ëª© ë°œê²¬! (ì»¬ëŸ¼ëª… ë°©ì‹)")
                return {"korean_name": ko}
        
        # ì»¬ëŸ¼ëª…ì´ ì—†ëŠ” ê²½ìš° ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼ (0:ë²ˆí˜¸, 1:êµ­ëª…, 2:í•™ëª… ê°€ì •)
        elif len(csv_df.columns) >= 3:
            # ì„¸ ë²ˆì§¸ ì»¬ëŸ¼(ì¸ë±ìŠ¤ 2)ì´ í•™ëª…, ë‘ ë²ˆì§¸ ì»¬ëŸ¼(ì¸ë±ìŠ¤ 1)ì´ êµ­ëª…
            sci_col = csv_df.iloc[:, 2].astype(str).str.strip().str.lower()
            mask = sci_col == sci.strip().lower()
            if mask.any():
                ko = csv_df.iloc[mask.idxmax(), 1]  # ì²« ë²ˆì§¸ ë§¤ì¹˜ì˜ êµ­ëª…
                log("  - CSV ì¼ì¹˜ í•­ëª© ë°œê²¬! (ì¸ë±ìŠ¤ ë°©ì‹)")
                return {"korean_name": ko}
        
        log(f"  - CSVì—ì„œ '{sci}' ì°¾ì§€ ëª»í•¨")
        return None
        
    except Exception as e:
        log(f"  - CSV ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None

# -------------- ì´ë¦„ ë³´ì™„(ìœ„í‚¤â†’CSVâ†’Gemini) --------------

def resolve_names(res: Dict, wiki_info, csv_df, log):
    common = res.get('common_name') or 'N/A'
    sci    = res.get('scientific_name') or 'N/A'
    order  = res.get('order')  or 'N/A'
    family = res.get('family') or 'N/A'
    korean = 'N/A'; src = 'N/A'; csv_used = False

    # 1ë‹¨ê³„: Wikipedia ìš°ì„  í™•ì¸
    if wiki_info:
        common = wiki_info.get('common_name', common)
        korean = wiki_info.get('korean_name', korean)
        src = 'Wikipedia'
        
        # Wikipediaì—ì„œ í•œêµ­ëª…ì´ *ë¡œ ì‹œì‘í•˜ë©´ (ì¦‰, í•œêµ­ì–´ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´) CSVë¡œ ë³´ì™„
        if korean.startswith('*'):
            log("  - Wikipedia í•œêµ­ëª… ì—†ìŒ, CSV ë³´ì™„ ì‹œë„...")
            csv_info = csv_lookup(csv_df, sci, log)
            if csv_info:
                korean = csv_info['korean_name']
                src = 'Wikipedia+CSV'
                csv_used = True
    
    # 2ë‹¨ê³„: Wikipedia ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í–ˆìœ¼ë©´ CSV ì§ì ‘ ì¡°íšŒ
    if not wiki_info or korean == 'N/A' or korean.startswith('*'):
        log("  - CSVì—ì„œ ì§ì ‘ ì¡°íšŒ...")
        csv_info = csv_lookup(csv_df, sci, log)
        if csv_info:
            korean = csv_info['korean_name']
            if not wiki_info:
                src = 'CSV'
            else:
                src = 'CSV (Wikipedia ë³´ì™„)'
            csv_used = True

    # 3ë‹¨ê³„: ëª¨ë“  ê²€ì¦ ì‹¤íŒ¨ì‹œ Gemini ê²°ê³¼ ì‚¬ìš©
    if korean == 'N/A' or korean.startswith('*'):
        korean = f"*{common}" if common != 'N/A' else 'ë¯¸ì‹ë³„'
        if src == 'N/A':
            src = 'Gemini (ê²€ì¦ ì‹¤íŒ¨)'
        else:
            src += ' (êµ­ëª… ë¯¸í™•ì¸)'
    
    return korean, common, sci, order, family, src, csv_used

# --------------------- ë¡œê·¸ ìƒì„± ---------------------

def create_logs(log_dir: str, obs: List[Dict], src_dir: str, log):
    if not obs:
        log("- ë¡œê·¸ë¥¼ ìƒì„±í•  ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."); return
    os.makedirs(log_dir, exist_ok=True)
    uniq = {o['scientific_name'] for o in obs if o['scientific_name'] != 'N/A'}

    with open(os.path.join(log_dir,'log_chronological.txt'),'w',encoding='utf-8') as f:
        f.write('='*50+'\nì‹œê°„ìˆœ ìë™ íƒì¡° ê¸°ë¡\n'+'='*50+'\n')
        f.write(f"ê¸°ë¡ ìƒì„±: {datetime.now():%Y-%m-%d %H:%M:%S}\nëŒ€ìƒ í´ë”: {os.path.abspath(src_dir)}\n")
        f.write(f"ì²˜ë¦¬ ì‚¬ì§„: {len(obs)}ê°œ\nê´€ì°° ì¢…: {len(uniq)}ì¢…\n"+'='*50+'\n\n')
        for o in obs:
            ts = o['datetime'].strftime('%Y-%m-%d %H:%M:%S') if o['datetime'] else 'ì‹œê°„ ì •ë³´ ì—†ìŒ'
            f.write(f"â–¶ {ts}\n  - êµ­ëª…: {o['korean_name']}\n  - ì˜ë¬¸ëª…: {o['common_name']}\n  - í•™ëª…: {o['scientific_name']}\n  - ë¶„ë¥˜: {o['taxonomy_str']}\n  - íŒŒì¼: {o['new_filename']}\n"+'-'*50+'\n')

    uniq_map = {o['scientific_name']:o for o in obs if o['scientific_name']!='N/A'}
    sorted_obs = sorted(uniq_map.values(), key=lambda x:(x['taxonomy'].get('order','zzz'),x['taxonomy'].get('family','zzz')))
    with open(os.path.join(log_dir,'log_taxonomic.txt'),'w',encoding='utf-8') as f:
        f.write('='*50+'\në¶„ë¥˜í•™ì  ì²´í¬ë¦¬ìŠ¤íŠ¸\n'+'='*50+'\n')
        f.write(f"ê¸°ë¡ ìƒì„±: {datetime.now():%Y-%m-%d %H:%M:%S}\nì´ ì¢…ìˆ˜: {len(sorted_obs)}ì¢…\n"+'='*50+'\n')
        cur_order=cur_family=''
        for o in sorted_obs:
            order=o['taxonomy'].get('order','ì •ë³´ ì—†ìŒ'); family=o['taxonomy'].get('family','ì •ë³´ ì—†ìŒ')
            if order!=cur_order: cur_order=order; f.write(f"\n[ëª©] {order}\n"); cur_family=''
            if family!=cur_family: cur_family=family; f.write(f"  [ê³¼] {family}\n")
            f.write(f"    - {o['korean_name']} ({o['common_name']})\n")
    log("  - ë¡œê·¸ íŒŒì¼ ìƒì„± ì™„ë£Œ.")

# -------------------- ë©”ì¸ í•¨ìˆ˜ --------------------

def process_all_images(cfg: Dict):
    log      = cfg['log_callback']
    yolo     = cfg['yolo_model']
    gemini   = cfg['gemini_model']
    wiki     = cfg['wiki_wiki']
    csv_df   = cfg.get('csv_db')

    src_dir  = cfg['target_folder']
    out_dir  = os.path.join(src_dir,'processed_birds_final'); os.makedirs(out_dir,exist_ok=True)
    log_dir  = os.path.join(out_dir,'íƒì¡°ê¸°ë¡')

    RESIZE=(768,768); CONF=0.25; DELAY=4
    RAW_EXT=('.orf','.cr2','.cr3','.nef','.arw','.dng','.raf','.rw2')

    observations=[]
    log(f"ëŒ€ìƒ: {os.path.abspath(src_dir)} â†’ ì¶œë ¥: {os.path.abspath(out_dir)}")
    
    # CSV ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    if csv_df is not None:
        log(f"CSV ë°ì´í„°ë² ì´ìŠ¤: í™œì„±í™” ({len(csv_df)}ê°œ ë ˆì½”ë“œ)")
    else:
        log("CSV ë°ì´í„°ë² ì´ìŠ¤: ë¹„í™œì„±í™”")

    for fname in os.listdir(src_dir):
        if not fname.lower().endswith(('.jpg','.jpeg')): continue
        src_path=os.path.join(src_dir,fname); log(f"\n- {fname} ì²˜ë¦¬ ì¤‘")
        
        try:
            yres=yolo(src_path,verbose=False)
            birds=[{'box':b.xyxy[0].cpu().numpy(),'conf':float(b.conf[0])} for b in yres[0].boxes if yolo.names[int(b.cls[0])]=='bird' and float(b.conf[0])>=CONF]
            if not birds: log("  - ìƒˆ ì—†ìŒ"); continue
            
            best=max(birds,key=lambda x:x['conf']); log(f"  - ìƒˆ íƒì§€! ({best['conf']:.2f})")
            
            with Image.open(src_path) as im:
                dt=get_photo_datetime(im)
                crop=im.crop(tuple(best['box'])).resize(RESIZE)
            
            # ì‚¬ì§„ì˜ ì´¬ì˜ ë‚ ì§œì—ì„œ ì›”/ì¼ ì •ë³´ ì¶”ì¶œ
            if dt:
                month_day = dt.strftime("%B %d")  # "June 16" í˜•íƒœ
                date_context = f" on {month_day}"
                seasonal_hint = f" Consider the seasonal migration patterns and breeding cycles typical for this time of year ({month_day})."
            else:
                date_context = ""
                seasonal_hint = ""
            
            # ë‚ ì§œ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_with_date = (f"Act as an expert ornithologist specializing in the avifauna of {cfg['photo_location']}. "
                              f"The following is a cropped image of a bird taken in {cfg['photo_location']}{date_context}."
                              f"{seasonal_hint} "
                              "Respond in JSON with 'common_name','scientific_name','order','family'. If uncertain set nulls.")
            
            log("  - Gemini API ë¶„ì„ ìš”ì²­...")
            
            # ë‹¤ì–‘í•œ ë³€í˜• ì´ë¯¸ì§€ ìƒì„±ìœ¼ë¡œ ì¸ì‹ë¥  í–¥ìƒ
            variations = [
                crop,                                      # ì›ë³¸
                crop.transpose(Image.ROTATE_90),           # 90ë„
                crop.transpose(Image.ROTATE_270),          # -90ë„ (270ë„)
                crop.transpose(Image.FLIP_LEFT_RIGHT),     # ì¢Œìš° ë°˜ì „
                crop.transpose(Image.FLIP_TOP_BOTTOM)      # ìƒí•˜ ë°˜ì „
            ]

            response = gemini.generate_content(
                [prompt_with_date] + variations,
                generation_config={"response_mime_type": "application/json"}
            )
            
            log(f"  - API ë”œë ˆì´ ({DELAY}ì´ˆ)..."); time.sleep(DELAY)
            res = json.loads(response.text)
            
            gemini_common = res.get('common_name')
            gemini_sci = res.get('scientific_name')
            
            if not gemini_common and not gemini_sci:
                log("  - Gemini ì‹ë³„ ì‹¤íŒ¨"); continue
            
            # Wikipedia ìš°ì„  ì¡°íšŒ
            wiki_info = wiki_lookup(wiki, gemini_common, gemini_sci, log)
            
            # ì´ë¦„ í•´ê²° (Wikipedia â†’ CSV â†’ Gemini ìˆœ)
            korean, common, sci, order, family, src, csv_used = resolve_names(res, wiki_info, csv_df, log)
            
            log(f"  - ìµœì¢… ì¶œì²˜: {src}")
            log(f"  - ìµœì¢… ê²°ê³¼: {korean} | {common} ({sci})")
            
            taxonomy_str = f"ëª©: {order}, ê³¼: {family}"
            taxonomy_dict = {"order": order, "family": family}
            
        except Exception as e:
            log(f"  ! ë¶„ì„ ì˜¤ë¥˜: {e}"); continue
        
        try:
            # íŒŒì¼ëª… ìƒì„±
            date_prefix = dt.strftime('%Y%m%d_%H%M%S_') if dt else ""
            if not korean.startswith('*'):
                base_name = f"{date_prefix}{sanitize_filename(korean)}_{sanitize_filename(common)}"
            else:
                base_name = f"{date_prefix}{sanitize_filename(common)}"
            
            new_fname = f"{base_name}.jpg"
            new_path = os.path.join(out_dir, new_fname)
            
            # ì¤‘ë³µ ë°©ì§€
            counter = 1
            while os.path.exists(new_path):
                new_fname = f"{base_name}_{counter}.jpg"
                new_path = os.path.join(out_dir, new_fname)
                counter += 1
            
            # JPG ë³µì‚¬
            shutil.copy2(src_path, new_path)
            log(f"  >> JPG ì €ì¥: {new_fname}")
            
            # RAW íŒŒì¼ ì°¾ì•„ì„œ ë³µì‚¬
            base_fname = os.path.splitext(fname)[0]
            for ext in RAW_EXT:
                raw_path = os.path.join(src_dir, f"{base_fname}{ext}")
                if os.path.exists(raw_path):
                    raw_new_name = f"{os.path.splitext(new_fname)[0]}{ext}"
                    raw_new_path = os.path.join(out_dir, raw_new_name)
                    shutil.copy2(raw_path, raw_new_path)
                    log(f"  >> RAW ì €ì¥: {raw_new_name}")
                    break
            
            # ê´€ì°° ê¸°ë¡ ì¶”ê°€
            observations.append({
                'datetime': dt,
                'new_filename': new_fname,
                'common_name': common,
                'korean_name': korean,
                'scientific_name': sci,
                'taxonomy': taxonomy_dict,
                'taxonomy_str': taxonomy_str,
                'csv_used': csv_used
            })
            
        except Exception as e:
            log(f"  ! íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    
    # ë¡œê·¸ ìƒì„±
    create_logs(log_dir, observations, src_dir, log)
    
    # CSV ì‚¬ìš© í†µê³„
    csv_count = sum(1 for o in observations if o.get('csv_used'))
    log(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
    log(f"  - ì´ ì²˜ë¦¬: {len(observations)}ê°œ")
    log(f"  - CSV í™œìš©: {csv_count}ê°œ")
    log(f"  - ê³ ìœ  ì¢…: {len(set(o['scientific_name'] for o in observations if o['scientific_name'] != 'N/A'))}ì¢…")